wandb:
  entity: null
#  run_id: askkz9i2
  resume: 'auto'

experiment:
    project: "training"
    name: "geouni-512x512-0130-t2i-mmu-debug"
    output_dir: "outputs/geouni-512x512-0130-t2i-mmu-debug"
    save_every: 500
    generate_every: 10
    log_every: 50
    log_grad_norm_every: 500
    checkpoints_total_limit: 1
    resume_from_checkpoint: 'latest' # 'latest'

model:
    vq_model:
        type: "geo"
        vq_model_config: 
            double_z: False
            z_channels: 13
            resolution: 512
            in_channels: 3
            out_ch: 3
            ch: 128
            ch_mult: [1,2,2,2,4,4]  # num_down = len(ch_mult)-1
            num_res_blocks: 4
        pretrained_model_path: "/lustre/home/2001110054/GEO-Open-MAGVIT2/outputs/expr_1120_mask_down32_z13/ckpt/epoch=184-step=68820.ckpt"

        
    geouni:
        load_from_geouni: false
        pretrained_model_path: null
        vocab_size: 159866
        llm_vocab_size: 151665
        llm_model_path: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' # 'Qwen/Qwen2.5-Math-1.5B'
        codebook_size: 8192
        num_vq_tokens: 256
        num_new_special_tokens: 9  # <|soi|> <|eoi|> <|t2i|> <|formalization|> <|reasoning|> <|mix|> <answer> </answer> [PAD]

dataset:
    combined_loader_mode: "max_size_cycle"
    params:
        t2i_image_folder: data/formalgeo7k/formalgeo7k_v2
        t2i_json_path: data/formalgeo7k/formalgeo7k_v2/custom_json/t2i_overfit/train.json
        formalization_image_folder: data/formalgeo7k/formalgeo7k_v2
        formalization_json_path: data/formalgeo7k/formalgeo7k_v2/custom_json/qa_structure_only/qa_structure_only_minitrain.json
        reasoning_image_folder: data/formalgeo7k/formalgeo7k_v2
        reasoning_json_path: data/formalgeo7k/formalgeo7k_v2/custom_json/qa_structure_only/qa_structure_only_minitrain.json
        mixing_image_folder: data/formalgeo7k/formalgeo7k_v2
        mixing_json_path: null
        t2i_validation_json_path: data/formalgeo7k/formalgeo7k_v2/custom_json/t2i_overfit/train.json
        mmu_validation_json_path: data/formalgeo7k/formalgeo7k_v2/custom_json/qa_structure_only/qa_structure_only_minitrain.json
        num_workers: 16
        pin_memory: True
        persistent_workers: True

    preprocessing:
        max_seq_length: 800
        resolution: 512

optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 5e-5
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.01
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 200

training:
    gradient_accumulation_steps: 1
    batch_size_t2i: 2
    batch_size_formalization: 2
    batch_size_reasoning: 2
    batch_size_mixing: 2
    num_train_epochs: 10000000
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    max_train_steps: 50000 # to be determined according to the scale of high-quality dataset
    max_grad_norm: null
    t2i_coeff: 1.0
    formalization_coeff: 1.0
    reasoning_coeff: 1.0
    mixing_coeff: 1.0